<!doctype html>
<html>

<head>

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Andreea Dogaru's Homepage</title>
  <link href="css/about.css" rel="stylesheet" type="text/css">
  <style type="text/css">
  </style>
</head>

<body alink="#282727" vlink="#9b9b9b" link="#9b9b9b">
  <!-- Header content -->
  <header>
    <div class="profilePhoto">
      <!-- Profile photo -->
      <img src="images/profile_pic.png" alt="sample" width="259">
    </div>
    <!-- Identity details -->
    <section class="profileHeader">
      <h1>Andreea Dogaru</h1>
      <h3>PhD Candidate</h3>
      <hr>
      <p>I am a researcher in the <a href="https://twitter.com/cogcovi" target="_blank">
          Cognitive Computer Vision Lab</a> within the <a href="https://www.lgdv.tf.fau.de/" target="_blank">
          Chair of Visual Computing</a> at <a href="https://www.fau.de/" target="_blank">
          Friedrich-Alexander-Universität Erlangen-Nürnberg</a>, advised by <a href="https://eggerbernhard.ch/"
          target="_blank">Prof. Dr. Bernhard Egger</a>.
        <br> My research interests are within Computer Vision and Computer Graphics fields, with a focus on Neural Scene
        Representations, 3D Reconstruction, and Shape Modelling.
        <br> Before joining FAU, I obtained my M. Sc. in Data Science from the
        <a href="https://www.skoltech.ru/en/" target="_blank">Skolkovo Institute of Science and Technology</a>
        and collaborated with <a href="https://research.samsung.com/" target="_blank">
          Samsung AI Center</a> on 3D Computer Vision applications.
        <br>
      </p>
    </section>
    <!-- Links to Social network accounts -->
    <aside class="socialNetworkNavBar">
      <div class="socialNetworkNav">
        <!-- Add a Anchor tag with nested img tag here -->
        <a href="mailto:andreea.dogaru@fau.de">
          <img src="images/mail.png" alt="sample" width="30"></a>
      </div>
      <div class="socialNetworkNav">
        <!-- Add a Anchor tag with nested img tag here -->
        <a href="https://github.com/AndreeaDogaru" target="_blank">
          <img src="images/github.png" alt="sample" width="30"></a>
      </div>
      <div class="socialNetworkNav">
        <a href="https://scholar.google.com/citations?user=HF1pWaAAAAAJ&hl=en" target="_blank">
          <!-- Add a Anchor tag with nested img tag here -->
          <img src="images/gscholar.png" alt="sample" width="30">
        </a>
      </div>
      <div class="socialNetworkNav">
        <!-- Add a Anchor tag with nested img tag here -->
        <a href="https://twitter.com/andreead_a" target="_blank">
          <img src="images/twitter.png" alt="sample" width="30"></a>
      </div>

      <div class="socialNetworkNav">
        <a href="https://www.linkedin.com/in/andreea-dogaru" target="_blank">
          <img src="images/linkedin.png" alt="sample" width="30"> </a>
      </div>
    </aside>
  </header>
  <!-- content -->
  <section class="mainContent">
    <!-- Contact details -->

    <!-- Previous experience details -->

    <section class="section2">
      <h2 class="sectionTitle">Publications</h2>
      <hr class="sectionTitleRule">
      <hr class="sectionTitleRule2">

      <section class="section2">

        <!-- SphereGuided  -->
        <div class="sectionContent" style="padding:0px;vertical-align:middle">
          <img src="Gen3DSR/images/teaser.png" style="height: 100%; width: 100%; object-fit: contain" alt="">
        </div>
        <section class="section2Content">
          <h2 class="sectionContentTitle"> Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single
            View </h2>
          <h3 class="sectionContentSubTitle">
            <span style="color:purple">Andreea&nbsp;Dogaru</span>,
            Mert&nbsp;Özer,
            <a href="https://eggerbernhard.ch/" target="_blank">Bernhard Egger</a>
          </h3>
          <h3 class="sectionContentSubTitle">Preprint - <b style="color:purple">arXiv&nbsp;2024</b></h3>
        </section>
        <aside class="externalResourcesNav">

          <div class="dropdown"><a href="https://andreeadogaru.github.io/Gen3DSR/" target="_blank">Project Page</a>
          </div>
          <div class="dropdown"> <span>Abstract</span>
            <div class="dropdown-content">
              <p style="text-align:left;"> Single-view 3D reconstruction is currently approached from two dominant
                perspectives: reconstruction of scenes with limited diversity using 3D data supervision or
                reconstruction of diverse singular objects using large image priors. However, real-world
                scenarios are far more complex and exceed the capabilities of these methods. We
                therefore propose a hybrid method following a divide-and-conquer strategy. We first
                process the scene holistically, extracting depth and semantic information, and then
                leverage a single-shot object-level method for the detailed reconstruction of
                individual components. By following a compositional processing approach, the overall
                framework achieves full reconstruction of complex 3D scenes from a single image. We
                purposely design our pipeline to be highly modular by carefully integrating specific
                procedures for each processing step, without requiring an end-to-end training of the
                whole system. This enables the pipeline to naturally improve as future methods can
                replace the individual modules. We demonstrate the reconstruction performance of our
                approach on both synthetic and real-world scenes, comparing favorable against prior
                works. </p>
            </div>
          </div>
          <div class="dropdown"><a href="https://arxiv.org/pdf/2404.03421.pdf" target="_blank">Paper</a>
          </div>
          <div class="dropdown"><a href="https://github.com/AndreeaDogaru/Gen3DSR" target="_blank">Code</a></div>
        </aside>

      </section>

      <br>
      
      <section class="section2">

        <!-- RANRAC  -->
        <div class="sectionContent" style="padding:0px;vertical-align:middle">
          <img src="images/teaser_ranrac.svg" style="height: 100%; width: 100%; object-fit: contain" alt="">
        </div>
        <section class="section2Content">
          <h2 class="sectionContentTitle"> RANRAC: Robust Neural Scene Representations via Random Ray Consensus </h2>
          <h3 class="sectionContentSubTitle">
            <a href="https://bennobuschmann.com/" target="_blank">Benno&nbsp;Buschmann</a>,
            <span style="color:purple">Andreea&nbsp;Dogaru</span>,
            <a href="https://graphics.tudelft.nl/~eisemann/" target="_blank">Elmar&nbsp;Eisemann</a>,
            <a href="https://scholar.google.de/citations?user=NexBuB8AAAAJ&hl=de"
              target="_blank">Michael&nbsp;Weinmann</a>,
            <a href="https://eggerbernhard.ch/" target="_blank">Bernhard&nbsp;Egger</a>
          </h3>
          <h3 class="sectionContentSubTitle">European Conference on Computer Vision - <b
              style="color:purple">ECCV&nbsp;2024</b></h3>
        </section>
        <aside class="externalResourcesNav">

          <div class="dropdown"><a href="https://bennobuschmann.com/ranrac/" target="_blank">Project Page</a></div>
          <div class="dropdown"> <span>Abstract</span>
            <div class="dropdown-content">
              <p style="text-align:left;"> Learning-based scene representations such as neural radiance fields or light
                field networks, that rely on fitting a scene model to image observations, commonly
                encounter challenges in the presence of inconsistencies within the images caused by
                occlusions, inaccurately estimated camera parameters or effects like lens flare. To
                address this challenge, we introduce RANdom RAy Consensus (RANRAC), an efficient
                approach to eliminate the effect of inconsistent data, thereby taking inspiration from
                classical RANSAC based outlier detection for model fitting. In contrast to the
                down-weighting of the effect of outliers based on robust loss formulations, our approach
                reliably detects and excludes inconsistent perspectives, resulting in clean images
                without floating artifacts. For this purpose, we formulate a fuzzy adaption of the
                RANSAC paradigm, enabling its application to large scale models. We interpret the
                minimal number of samples to determine the model parameters as a tunable
                hyperparameter, investigate the generation of hypotheses with data-driven models, and analyse
                the validation of hypotheses in noisy environments. We demonstrate the
                compatibility and potential of our solution for both photo-realistic robust multi-view
                reconstruction from real-world images based on neural radiance fields and for single-shot
                reconstruction based on light-field networks. In particular, the results indicate significant
                improvements compared to state-of-the-art robust methods for novel-view synthesis on both
                synthetic and captured scenes with various inconsistencies including occlusions, noisy
                camera pose estimates, and unfocused perspectives. The results further indicate
                significant improvements for single-shot reconstruction from occluded images.
              </p>
            </div>
          </div>
          <div class="dropdown"><a href="https://arxiv.org/pdf/2312.09780.pdf" target="_blank">Paper</a>
          </div>
        </aside>
      </section>

      <br>

      <section class="section2">

        <!-- NeuralHaircut  -->
        <div class="sectionContent" style="padding:0px;vertical-align:middle">
          <img src="images/teaser_neuralhaircut.png" style="height: 100%; width: 100%; object-fit: contain" alt="">
        </div>
        <section class="section2Content">
          <h2 class="sectionContentTitle"> Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction</h2>
          <h3 class="sectionContentSubTitle">
            <a href="https://ncs.is.mpg.de/person/vsklyarova" target="_blank">Vanessa&nbsp;Sklyarova</a>,
            <a href="https://scholar.google.com/citations?user=67HtbJUAAAAJ" target="_blank">Jenya&nbsp;Chelishev</a>,
            <span style="color:purple">Andreea&nbsp;Dogaru</span>,
            Igor&nbsp;Medvedev,
            <a href="https://egorzakharov.github.io/" target="_blank">Egor&nbsp;Zakharov</a>,
            <a href="https://scholar.google.com/citations?user=gYYVokYAAAAJ&hl=en"
              target="_blank">Victor&nbsp;Lempitsky</a>
          </h3>
          <h3 class="sectionContentSubTitle">International Conference on Computer Vision - <b
              style="color:purple">ICCV&nbsp;2023</b></h3>
        </section>
        <aside class="externalResourcesNav">

          <div class="dropdown"><a href="https://samsunglabs.github.io/NeuralHaircut/" target="_blank">Project Page</a>
          </div>
          <div class="dropdown"> <span>Abstract</span>
            <div class="dropdown-content">
              <p style="text-align:left;"> We propose an approach that can accurately reconstruct hair geometry at a
                strand level from a monocular video or multi-view images captured in uncontrolled lighting conditions.
                Our
                method has two stages, with the first stage performing joint reconstruction of coarse hair and bust
                shapes
                and hair orientation using implicit volumetric representations. The second stage then estimates a
                strand-level hair reconstruction by reconciling in a single optimization process the coarse volumetric
                constraints with hair strand and hairstyle priors learned from the synthetic data. To further increase
                the
                reconstruction fidelity, we incorporate image-based losses into the fitting process using a new
                differentiable renderer. The combined system, named Neural Haircut, achieves high realism and
                personalization of the reconstructed hairstyles. </p>
            </div>
          </div>
          <div class="dropdown"><a href="https://arxiv.org/pdf/2306.05872.pdf" target="_blank">Paper</a>
          </div>
          <div class="dropdown"><a href="https://github.com/SamsungLabs/NeuralHaircut" target="_blank">Code</a></div>
        </aside>

      </section>

      <br>

      <section class="section2">
        <!-- SphereGuided  -->
        <div class="sectionContent" style="padding:0px;vertical-align:middle">
          <img src="SphereGuided/images/teaser.jpg" style="height: 100%; width: 100%; object-fit: contain" alt="">
        </div>
        <section class="section2Content">
          <h2 class="sectionContentTitle"> Sphere-Guided Training of Neural Implicit Surfaces </h2>
          <h3 class="sectionContentSubTitle">
            <span style="color:purple">Andreea&nbsp;Dogaru</span>,
            <a href="https://reality.tf.fau.de/staff/t.ardelean.html" target="_blank">Andrei-Timotei&nbsp;Ardelean</a>,
            <a href="https://scholar.google.com/citations?user=aJyxXoMAAAAJ&hl=en"
              target="_blank">Savva&nbsp;Ignatyev</a>,
            <a href="https://egorzakharov.github.io/" target="_blank">Egor&nbsp;Zakharov</a>,
            <a href="https://faculty.skoltech.ru/people/evgenyburnaev" target="_blank">Evgeny&nbsp;Burnaev</a>
          </h3>
          <h3 class="sectionContentSubTitle">Conference on Computer Vision and Pattern Recognition - <b
              style="color:purple">CVPR&nbsp;2023</b></h3>
        </section>
        <aside class="externalResourcesNav">

          <div class="dropdown"><a href="https://andreeadogaru.github.io/SphereGuided/" target="_blank">Project Page</a>
          </div>
          <div class="dropdown"> <span>Abstract</span>
            <div class="dropdown-content">
              <p style="text-align:left;"> In recent years, neural distance functions trained via volumetric ray
                marching
                have been widely adopted for multi-view 3D reconstruction. These methods, however, apply the ray
                marching
                procedure for the entire scene volume, leading to reduced sampling efficiency and, as a result, lower
                reconstruction quality in the areas of high-frequency details. In this work, we address this problem via
                joint training of the implicit function and our new coarse sphere-based surface reconstruction. We use
                the
                coarse representation to efficiently exclude the empty volume of the scene from the volumetric ray
                marching procedure without additional forward passes of the neural surface network, which leads to an
                increased fidelity of the reconstructions compared to the base systems. We evaluate our approach by
                incorporating it into the training procedures of several implicit surface modeling methods and observe
                uniform improvements across both synthetic and real-world datasets.</p>
            </div>
          </div>
          <div class="dropdown"><a href="https://arxiv.org/pdf/2209.15511.pdf" target="_blank">Paper</a>
          </div>
          <div class="dropdown"><a href="https://github.com/AndreeaDogaru/SphereGuided" target="_blank">Code</a></div>
        </aside>

        <br>

      </section>
      <hr>

    </section>
    <footer>
      <p class="footerDisclaimer">Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for
        the
        website template.<br>
        Icons included in the page are from <a href="https://www.flaticon.com/authors/pixel-perfect"
          target="_blank">Flaticon</a>.<span></span></p>
      <p class="footerNote"></p>
    </footer>
</body>

</html>